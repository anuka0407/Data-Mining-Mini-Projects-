tfidf_train_matrix <- as.matrix(tfidf_train)
summary(tfidf_train_matrix)%>%kable()
#matrix of test tfidf
tfidf_test_matrix <- as.matrix(tfidf_test)
pca_train = prcomp(tfidf_train_matrix, scale=TRUE)
plot(pca_train)
#dim(pca_train$x)
# [1] 2500 1141
#look at the loadings
#pca_train$rotation[order(abs(pca_train$rotation[,1]),decreasing=TRUE),1][1:1000]
#pca_train$rotation[order(abs(pca_train$rotation[,1]),decreasing=TRUE),2][1:1000]
summary(pca_train)$importance[3,]
#pc194 explains more than 50% of the variance in  the data
tfidf_train_matrix <- pca_train$x[,1:194]
tfidf_train_matrix <- cbind(tfidf_train_matrix, class_labels_train)
loading_train <- pca_train$rotation[,1:194]
# multiply to get a test matrix with the principal component values
tfidf_test_matrix_pca <- scale(tfidf_test_matrix) %*% loading_train
tfidf_test_matrix_pca <- as.data.frame(tfidf_test_matrix_pca)
tfidf_train_matrix2  <- as.data.frame(tfidf_train_matrix)
for (name in names(tfidf_train_matrix2 )){
if (name == "class_labels_train"){
next
}else{
tfidf_train_matrix2 [[name]] <- as.numeric(as.character(tfidf_train_matrix2 [[name]]))
}
}
tfidf_train_matrix2$class_labels_train <- as.factor(tfidf_train_matrix2$class_labels_train)
# read in the files and store them as a list
all_files_train <- lapply(file_list_train, readerPlain)
getwd()
# read in train data and create DTM
author_names_train <- dir("./ReutersC50/C50train")
file_list_train <- NULL
class_labels_train <- NULL
for (name in author_names_train){
file_list_train <- c(file_list_train, Sys.glob(paste0('./ReutersC50/C50train/', name,'/*.txt')))
class_labels_train <- c(class_labels_train, rep(name, each = length(Sys.glob(paste0('./ReutersC50/C50train/', name,'/*.txt')))))
}
# define the function that will read in the files
readerPlain = function(fname){
readPlain(elem = list(content = readLines(fname)),
id = fname, language = 'en') }
# read in the files and store them as a list
all_files_train <- lapply(file_list_train, readerPlain)
# give each file a representative name
file_names_train <- file_list_train %>%
strsplit("/") %>%
lapply(tail,n = 2) %>%
lapply(paste0, collapse = "") %>%
unlist
# create a dataframe with doc_id as author-article and text as the text in that article
text_vector_train <- NULL
for(i in 1:length(file_names_train)){
text_vector_train <- c(text_vector_train, paste0(content(all_files_train[[i]]), collapse = " "))
}
# dataframe with text and document_id
text_df_train <- data.frame(doc_id = file_names_train,
text = text_vector_train)
# convert the dataframe to a Corpus
train_corpus_raw <- VCorpus(DataframeSource(text_df_train))
# read in the test documents
author_names_test <- dir("./ReutersC50/C50test")
file_list_test <- NULL
class_labels_test <- NULL
for (name in author_names_test){
file_list_test <- c(file_list_test, Sys.glob(paste0('./ReutersC50/C50test/', name,'/*.txt')))
class_labels_test <- c(class_labels_test, rep(name, each = length(Sys.glob(paste0('./ReutersC50/C50test/', name,'/*.txt')))))
}
# read in the files and store them as a list
all_files_test <- lapply(file_list_test, readerPlain)
# give each file a representative name
file_names_test <- file_list_test %>%
strsplit("/") %>%
lapply(tail,n = 2) %>%
lapply(paste0, collapse = "") %>%
unlist
# create a dataframe with doc_id as author-article and text as the text in that article
text_vector_test <- NULL
for(i in 1:length(file_names_test)){
text_vector_test <- c(text_vector_test, paste0(content(all_files_test[[i]]), collapse = " "))
}
# dataframe with text and document_id
text_df_test <- data.frame(doc_id = file_names_test,
text = text_vector_test)
# convert the dataframe to a Corpus
test_corpus_raw <- VCorpus(DataframeSource(text_df_test))
```
**2.Pre-Processing both train and test data**
While dealing with text data, it is optimal to ignore numbers,punctuations,white spaces as they don't help much in gaining an insight into the patterns present in the text.It is also important to ignore the **Stop words**(words like is/an/the e.t.c) as they occur multiple times with no real information being added to the models.In this step, we have removed the stop words, punctuations, numbers to proceed with the analysis
```{r, message=FALSE, warning=FALSE}
#train data
# pre-processing to remove punctuations, spaces, etc.
train_corpus_preproc <- train_corpus_raw
train_corpus_preproc <- tm_map(train_corpus_preproc, content_transformer(tolower))
train_corpus_preproc <- tm_map(train_corpus_preproc, content_transformer(removeNumbers)) # remove numbers
train_corpus_preproc <- tm_map(train_corpus_preproc, content_transformer(removePunctuation)) # remove punctuation
train_corpus_preproc <- tm_map(train_corpus_preproc, content_transformer(stripWhitespace)) ## remove excess white-space
train_corpus_preproc <- tm_map(train_corpus_preproc, content_transformer(removeWords), stopwords("en")) # remove stop words
#test data
# pre-processing to remove punctuations, spaces, etc.
test_corpus_preproc <- test_corpus_raw
test_corpus_preproc <- tm_map(test_corpus_preproc, content_transformer(tolower)) # make everything lowercase
test_corpus_preproc <- tm_map(test_corpus_preproc, content_transformer(removeNumbers)) # remove numbers
test_corpus_preproc <- tm_map(test_corpus_preproc, content_transformer(removePunctuation)) # remove punctuation
test_corpus_preproc <- tm_map(test_corpus_preproc, content_transformer(stripWhitespace)) ## remove excess white-space
test_corpus_preproc <- tm_map(test_corpus_preproc, content_transformer(removeWords), stopwords("en")) # remove stop words
```
**3.Create TF-IDF matrix for both train and test data**
Next step in this process is to create a TF-IDF matrix of the corpus of documents that we have created. TF-IDF is a combination of TF(Term frequency) and IDF(Inverse document frequency). TF gives the number of times a word occurs in a document while IDF gives less weightage to words that occur in multiple documents and is not useful in identifying the style of anyone single author.
After the creation of the TF-IDF matrix, sparsing is one more step that is recommended while dealing with text data. **In sparsing, we remove terms that occur less frequently among all the documents**.This is generally decided by a threshold that is set heuristically based on the TF-IDF matrix. In this analysis, the threshold is set at **99%(words that are not present in 99% of the documents will be removed from the analysis)**
```{r, message=FALSE, warning=FALSE}
#train
# convert the corpus to a document term matrix
DTM_train <- DocumentTermMatrix(train_corpus_preproc)
# remove sparse terms from the DTM_train
DTM_train <- removeSparseTerms(DTM_train, 0.99)
#test
# convert the corpus to a document term matrix
DTM_test <- DocumentTermMatrix(test_corpus_preproc,
control = list(dictionary = Terms(DTM_train)))
# calculate the TF-IDF for each term in the DTM
tfidf_train <- weightTfIdf(DTM_train)
tfidf_test <- weightTfIdf(DTM_test)
X_train <- as.matrix(tfidf_train)
X_test <- as.matrix(tfidf_test)
```
**4.Dimensionality reduction**
Often while dealing with text data, we encounter with dimensionality problem. Due to the sheer volume of words present in any language, we often end up with thousands of words(columns) in the TF-IDF matrix. This makes it comptutationally heavy for any system to perform any modelling on the data. Dimensionality is the go to solution in these situations.
For this problem, i have used **PCA(Prinipal Component Analysis)** to reduce the number of dimensions in the data set.
After running PCA, a total of 350 components have been selected for further analysis as they explain about 50% of the total variaion in the data.
There is one interseting step that we perform during PCA. As we have 2 different datasets(train and test) to deal with, it is important to ensure that both the datasets have similar type of principal components that are aligned towards the same subspace respectively.
**To achieve that, we will use the loadings, rotations and other attributes from the PC object of the train data to align the components of the test data in a similar orientation as the train data.**
```{r, message=FALSE, warning=FALSE}
pca_train = prcomp(X_train, scale=TRUE)
# plot(pca_train)
# we will take the first 350 components because they explain 50% of the variance in the data
# summary(pca_train)$importance[3,]
X_train <- pca_train$x[,1:350]
X_train <- cbind(X_train, class_labels_train)
loading_train <- pca_train$rotation[,1:350]
# multiply to get a test matrix with the principal component values
X_test_pc <- scale(X_test) %*% loading_train
X_test_pc <- as.data.frame(X_test_pc)
rm(list = c("all_files_test", "all_files_train", "test_corpus_preproc", "train_corpus_preproc", "text_df_test", "text_df_train",
"author_names_test", "author_names_train", "file_list_test", "file_list_train", "i", "name", "text_vector_test",
"text_vector_train", "file_names_test", "file_names_train", "DTM_test", "DTM_train", "pca_train",
"test_corpus_raw", "train_corpus_raw", "tfidf_train", "tfidf_test"))
X_train <- as.data.frame(X_train)
for (name in names(X_train)){
if (name == "class_labels_train"){
next
}else{
X_train[[name]] <- as.numeric(as.character(X_train[[name]]))
}
}
X_train$class_labels_train <- as.factor(X_train$class_labels_train)
#
# plot(summary(pca_train)$importance[3,], main = "PCA Analysis Train", xlab = "Components",
#      ylab = "Cumulative % Variance Explained")
```
**2.Pre-Processing both train and test data**
While dealing with text data, it is optimal to ignore numbers,punctuations,white spaces as they don't help much in gaining an insight into the patterns present in the text.It is also important to ignore the **Stop words**(words like is/an/the e.t.c) as they occur multiple times with no real information being added to the models.In this step, we have removed the stop words, punctuations, numbers to proceed with the analysis
```{r, message=FALSE, warning=FALSE}
#train data
# pre-processing to remove punctuations, spaces, etc.
train_corpus_preproc <- train_corpus_raw
train_corpus_preproc <- tm_map(train_corpus_preproc, content_transformer(tolower))
train_corpus_preproc <- tm_map(train_corpus_preproc, content_transformer(removeNumbers)) # remove numbers
train_corpus_preproc <- tm_map(train_corpus_preproc, content_transformer(removePunctuation)) # remove punctuation
train_corpus_preproc <- tm_map(train_corpus_preproc, content_transformer(stripWhitespace)) ## remove excess white-space
train_corpus_preproc <- tm_map(train_corpus_preproc, content_transformer(removeWords), stopwords("en")) # remove stop words
#test data
# pre-processing to remove punctuations, spaces, etc.
test_corpus_preproc <- test_corpus_raw
test_corpus_preproc <- tm_map(test_corpus_preproc, content_transformer(tolower)) # make everything lowercase
test_corpus_preproc <- tm_map(test_corpus_preproc, content_transformer(removeNumbers)) # remove numbers
test_corpus_preproc <- tm_map(test_corpus_preproc, content_transformer(removePunctuation)) # remove punctuation
test_corpus_preproc <- tm_map(test_corpus_preproc, content_transformer(stripWhitespace)) ## remove excess white-space
test_corpus_preproc <- tm_map(test_corpus_preproc, content_transformer(removeWords), stopwords("en")) # remove stop words
```
**3.Create TF-IDF matrix for both train and test data**
Next step in this process is to create a TF-IDF matrix of the corpus of documents that we have created. TF-IDF is a combination of TF(Term frequency) and IDF(Inverse document frequency). TF gives the number of times a word occurs in a document while IDF gives less weightage to words that occur in multiple documents and is not useful in identifying the style of anyone single author.
After the creation of the TF-IDF matrix, sparsing is one more step that is recommended while dealing with text data. **In sparsing, we remove terms that occur less frequently among all the documents**.This is generally decided by a threshold that is set heuristically based on the TF-IDF matrix. In this analysis, the threshold is set at **99%(words that are not present in 99% of the documents will be removed from the analysis)**
```{r, message=FALSE, warning=FALSE}
#train
# convert the corpus to a document term matrix
DTM_train <- DocumentTermMatrix(train_corpus_preproc)
# remove sparse terms from the DTM_train
DTM_train <- removeSparseTerms(DTM_train, 0.99)
#test
# convert the corpus to a document term matrix
DTM_test <- DocumentTermMatrix(test_corpus_preproc,
control = list(dictionary = Terms(DTM_train)))
# calculate the TF-IDF for each term in the DTM
tfidf_train <- weightTfIdf(DTM_train)
tfidf_test <- weightTfIdf(DTM_test)
X_train <- as.matrix(tfidf_train)
X_test <- as.matrix(tfidf_test)
```
**4.Dimensionality reduction**
Often while dealing with text data, we encounter with dimensionality problem. Due to the sheer volume of words present in any language, we often end up with thousands of words(columns) in the TF-IDF matrix. This makes it comptutationally heavy for any system to perform any modelling on the data. Dimensionality is the go to solution in these situations.
For this problem, i have used **PCA(Prinipal Component Analysis)** to reduce the number of dimensions in the data set.
After running PCA, a total of 350 components have been selected for further analysis as they explain about 50% of the total variaion in the data.
There is one interseting step that we perform during PCA. As we have 2 different datasets(train and test) to deal with, it is important to ensure that both the datasets have similar type of principal components that are aligned towards the same subspace respectively.
**To achieve that, we will use the loadings, rotations and other attributes from the PC object of the train data to align the components of the test data in a similar orientation as the train data.**
```{r, message=FALSE, warning=FALSE}
pca_train = prcomp(X_train, scale=TRUE)
# plot(pca_train)
# we will take the first 350 components because they explain 50% of the variance in the data
# summary(pca_train)$importance[3,]
X_train <- pca_train$x[,1:350]
X_train <- cbind(X_train, class_labels_train)
loading_train <- pca_train$rotation[,1:350]
# multiply to get a test matrix with the principal component values
X_test_pc <- scale(X_test) %*% loading_train
X_test_pc <- as.data.frame(X_test_pc)
rm(list = c("all_files_test", "all_files_train", "test_corpus_preproc", "train_corpus_preproc", "text_df_test", "text_df_train",
"author_names_test", "author_names_train", "file_list_test", "file_list_train", "i", "name", "text_vector_test",
"text_vector_train", "file_names_test", "file_names_train", "DTM_test", "DTM_train", "pca_train",
"test_corpus_raw", "train_corpus_raw", "tfidf_train", "tfidf_test"))
X_train <- as.data.frame(X_train)
for (name in names(X_train)){
if (name == "class_labels_train"){
next
}else{
X_train[[name]] <- as.numeric(as.character(X_train[[name]]))
}
}
X_train$class_labels_train <- as.factor(X_train$class_labels_train)
#
# plot(summary(pca_train)$importance[3,], main = "PCA Analysis Train", xlab = "Components",
#      ylab = "Cumulative % Variance Explained")
```
**2.Pre-Processing both train and test data**
While dealing with text data, it is optimal to ignore numbers,punctuations,white spaces as they don't help much in gaining an insight into the patterns present in the text.It is also important to ignore the **Stop words**(words like is/an/the e.t.c) as they occur multiple times with no real information being added to the models.In this step, we have removed the stop words, punctuations, numbers to proceed with the analysis
#train data
# pre-processing to remove punctuations, spaces, etc.
train_corpus_preproc <- train_corpus_raw
train_corpus_preproc <- tm_map(train_corpus_preproc, content_transformer(tolower))
train_corpus_preproc <- tm_map(train_corpus_preproc, content_transformer(removeNumbers)) # remove numbers
train_corpus_preproc <- tm_map(train_corpus_preproc, content_transformer(removePunctuation)) # remove punctuation
train_corpus_preproc <- tm_map(train_corpus_preproc, content_transformer(stripWhitespace)) ## remove excess white-space
train_corpus_preproc <- tm_map(train_corpus_preproc, content_transformer(removeWords), stopwords("en")) # remove stop words
#test data
# pre-processing to remove punctuations, spaces, etc.
test_corpus_preproc <- test_corpus_raw
test_corpus_preproc <- tm_map(test_corpus_preproc, content_transformer(tolower)) # make everything lowercase
test_corpus_preproc <- tm_map(test_corpus_preproc, content_transformer(removeNumbers)) # remove numbers
test_corpus_preproc <- tm_map(test_corpus_preproc, content_transformer(removePunctuation)) # remove punctuation
test_corpus_preproc <- tm_map(test_corpus_preproc, content_transformer(stripWhitespace)) ## remove excess white-space
test_corpus_preproc <- tm_map(test_corpus_preproc, content_transformer(removeWords), stopwords("en")) # remove stop words
**3.Create TF-IDF matrix for both train and test data**
Next step in this process is to create a TF-IDF matrix of the corpus of documents that we have created. TF-IDF is a combination of TF(Term frequency) and IDF(Inverse document frequency). TF gives the number of times a word occurs in a document while IDF gives less weightage to words that occur in multiple documents and is not useful in identifying the style of anyone single author.
After the creation of the TF-IDF matrix, sparsing is one more step that is recommended while dealing with text data. **In sparsing, we remove terms that occur less frequently among all the documents**.This is generally decided by a threshold that is set heuristically based on the TF-IDF matrix. In this analysis, the threshold is set at **99%(words that are not present in 99% of the documents will be removed from the analysis)**
#train
# convert the corpus to a document term matrix
DTM_train <- DocumentTermMatrix(train_corpus_preproc)
# remove sparse terms from the DTM_train
DTM_train <- removeSparseTerms(DTM_train, 0.99)
#test
# convert the corpus to a document term matrix
DTM_test <- DocumentTermMatrix(test_corpus_preproc,
control = list(dictionary = Terms(DTM_train)))
# calculate the TF-IDF for each term in the DTM
tfidf_train <- weightTfIdf(DTM_train)
tfidf_test <- weightTfIdf(DTM_test)
X_train <- as.matrix(tfidf_train)
X_test <- as.matrix(tfidf_test)
**4.Dimensionality reduction**
Often while dealing with text data, we encounter with dimensionality problem. Due to the sheer volume of words present in any language, we often end up with thousands of words(columns) in the TF-IDF matrix. This makes it comptutationally heavy for any system to perform any modelling on the data. Dimensionality is the go to solution in these situations.
For this problem, i have used **PCA(Prinipal Component Analysis)** to reduce the number of dimensions in the data set.
After running PCA, a total of 350 components have been selected for further analysis as they explain about 50% of the total variaion in the data.
There is one interseting step that we perform during PCA. As we have 2 different datasets(train and test) to deal with, it is important to ensure that both the datasets have similar type of principal components that are aligned towards the same subspace respectively.
**To achieve that, we will use the loadings, rotations and other attributes from the PC object of the train data to align the components of the test data in a similar orientation as the train data.**
```{r, message=FALSE, warning=FALSE}
pca_train = prcomp(X_train, scale=TRUE)
# plot(pca_train)
# we will take the first 350 components because they explain 50% of the variance in the data
# summary(pca_train)$importance[3,]
X_train <- pca_train$x[,1:350]
X_train <- cbind(X_train, class_labels_train)
loading_train <- pca_train$rotation[,1:350]
# multiply to get a test matrix with the principal component values
X_test_pc <- scale(X_test) %*% loading_train
X_test_pc <- as.data.frame(X_test_pc)
rm(list = c("all_files_test", "all_files_train", "test_corpus_preproc", "train_corpus_preproc", "text_df_test", "text_df_train",
"author_names_test", "author_names_train", "file_list_test", "file_list_train", "i", "name", "text_vector_test",
"text_vector_train", "file_names_test", "file_names_train", "DTM_test", "DTM_train", "pca_train",
"test_corpus_raw", "train_corpus_raw", "tfidf_train", "tfidf_test"))
X_train <- as.data.frame(X_train)
for (name in names(X_train)){
if (name == "class_labels_train"){
next
}else{
X_train[[name]] <- as.numeric(as.character(X_train[[name]]))
}
}
X_train$class_labels_train <- as.factor(X_train$class_labels_train)
#
# plot(summary(pca_train)$importance[3,], main = "PCA Analysis Train", xlab = "Components",
#      ylab = "Cumulative % Variance Explained")
```
X_train$class_labels_train <- as.factor(X_train$class_labels_train)
class_labels_train
#load libraries
library(tidyverse)
library(corrplot)
library(gridExtra)
library(GGally)
library(LICORS)
library(skimr)
library(kableExtra)
library(dplyr)
library(factoextra)
library(arules)
library(arulesViz)
library(RColorBrewer)
library(gridExtra)
library(tidytext)
library(wordcloud2)
library(randomForest)
library(caret)
library(tm)
library(slam)
library(proxy)
library(tidytext)
library(gamlr)
library(SnowballC)
train_dirs = Sys.glob('./ReutersC50/C50train/*')
#train_dirs = train_dirs[c(43, 47)]
file_list_train = NULL
class_labels_train = NULL
for(author in test_dirs) {
files_to_add = Sys.glob(paste0(author, '/*.txt'))
file_list_train = append(file_list_train, files_to_add)
labels_test_train = append(labels_test_train, rep(author_name, length(files_to_add)))
}
for(author in train_dirs) {
files_to_add = Sys.glob(paste0(author, '/*.txt'))
file_list_train = append(file_list_train, files_to_add)
labels_test_train = append(labels_test_train, rep(author_name, length(files_to_add)))
}
for(author in train_dirs) {
files_to_add = Sys.glob(paste0(author, '/*.txt'))
file_list_train = append(file_list_train, files_to_add)
class_labels_train = append(class_labels_train, rep(author_name, length(files_to_add)))
}
for(author in train_dirs) {
files_to_add = Sys.glob(paste0(author, '/*.txt'))
file_list_train = append(file_list_train, files_to_add)
class_labels_train = append(class_labels_train, rep(author, length(files_to_add)))
}
# read in the files and store them as a list
all_files_train <- lapply(file_list_train, readerPlain)
# https://gist.github.com/jgscott/28d9d1287a0c3c1477e2113f6758d5ff
readerPlain = function(fname){
readPlain(elem=list(content=readLines(fname)),
id=fname, language='en') }
# read in the files and store them as a list
all_files_train <- lapply(file_list_train, readerPlain)
corpus_train = Corpus(DirSource(train_dirs))
#pre processing for train data
corpus_train = corpus_train %>% tm_map(., content_transformer(tolower)) %>%
tm_map(., content_transformer(removeNumbers)) %>%
tm_map(., content_transformer(removePunctuation)) %>%
tm_map(., content_transformer(stripWhitespace)) %>%
tm_map(., content_transformer(removeWords), stopwords("SMART")) %>%
tm_map(., content_transformer(removeWords), stopwords("en"))
##read in the test document
test_dirs<-Sys.glob('./ReutersC50/C50test/*')
file_list_test <- NULL
class_labels_test <- NULL
for(author in test_dirs) {
files_to_add = Sys.glob(paste0(author, '/*.txt'))
file_list_test = append(file_list_test, files_to_add)
class_labels_test = append(class_labels_test, rep(author, length(files_to_add)))
}
# read in the files and store them as a list
all_files_test <- lapply(file_list_test, readerPlain)
# read in the files and store them as a list
all_files_test <- lapply(file_list_test, readerPlain)
corpus_test<-Corpus(DirSource(test_dirs))
#pre processing for test data
corpus_test = corpus_test %>% tm_map(., content_transformer(tolower)) %>%
tm_map(., content_transformer(removeNumbers)) %>%
tm_map(., content_transformer(removePunctuation)) %>%
tm_map(., content_transformer(stripWhitespace)) %>%
tm_map(., content_transformer(removeWords), stopwords("SMART")) %>%
tm_map(., content_transformer(removeWords), stopwords("en"))
DTM_train<-DocumentTermMatrix(corpus_train)
DTM_train <- removeSparseTerms(DTM_train, 0.97)
DTM_test<-DocumentTermMatrix(corpus_test, control=list(dictionary=Terms(DTM_train)))
#calculate Term frequency matrix & IDF for each term in the DTM
tfidf_train <- weightTfIdf(DTM_train)
tfidf_test <- weightTfIdf(DTM_test)
#convert tfidf_train as matrix
tfidf_train_matrix <- as.matrix(tfidf_train)
tfidf_test_matrix <- as.matrix(tfidf_test)
pca_train = prcomp(tfidf_train_matrix, scale=TRUE)
plot(pca_train)
summary(pca_train)$importance[3,]
tfidf_train_matrix <- pca_train$x[,1:194]
tfidf_train_matrix <- cbind(tfidf_train_matrix, class_labels_train)
loading_train <- pca_train$rotation[,1:194]
# multiply to get a test matrix with the principal component values
tfidf_test_matrix_pca <- scale(tfidf_test_matrix) %*% loading_train
tfidf_test_matrix_pca <- as.data.frame(tfidf_test_matrix_pca)
tfidf_train_matrix2  <- as.data.frame(tfidf_train_matrix)
for (name in names(tfidf_train_matrix2 )){
if (name == "class_labels_train"){
next
}else{
tfidf_train_matrix2 [[name]] <- as.numeric(as.character(tfidf_train_matrix2 [[name]]))
}
}
tfidf_train_matrix2$class_labels_train <- as.factor(tfidf_train_matrix2$class_labels_train)
forest1<-randomForest(class_labels_train ~ .,
data = tfidf_train_matrix2,
ntree = 500)
predict1 <- predict(forest1, tfidf_test_matrix_pca, type = "response")
head(predict1)
answer <- as.data.frame(table(predict1, class_labels_test))
answer
answer$correct <- ifelse(answer$predict1==answer$class_labels_test, 1, 0)
answer <- as.data.frame(table(predict1, class_labels_test))
answer$correct <- ifelse(answer$predict1==answer$class_labels_test, 1, 0)
attributes(class_labels_test)
class(class_labels_test)
head(class_labels_test)
tfidf_test_matrix_pca$class_labels_test<-as.factor(tfidf_test_matrix_pca$class_labels_test)
# multiply to get a test matrix with the principal component values
tfidf_test_matrix_pca <- scale(tfidf_test_matrix) %*% loading_train
tfidf_test_matrix_pca <- as.data.frame(tfidf_test_matrix_pca)
tfidf_test_matrix_pca<-cbind(tfidf_test_matrix_pca, class_labels_test)
tfidf_test_matrix_pca2<-as.data.frame(tfidf_test_matrix_pca)
for (name in names(tfidf_test_matrix_pca2)){
if (name == "class_labels_test"){
next
}else{
tfidf_test_matrix_pca2 [[name]] <- as.numeric(as.character(tfidf_test_matrix_pca2 [[name]]))
}
}
tfidf_test_matrix_pca2$class_labels_test <- as.factor(tfidf_test_matrix_pca2$class_labels_test)
tfidf_train_matrix2  <- as.data.frame(tfidf_train_matrix)
for (name in names(tfidf_train_matrix2 )){
if (name == "class_labels_train"){
next
}else{
tfidf_train_matrix2 [[name]] <- as.numeric(as.character(tfidf_train_matrix2 [[name]]))
}
}
tfidf_train_matrix2$class_labels_train <- as.factor(tfidf_train_matrix2$class_labels_train)
# i have training matrix tfidf_train_matrix2 and test matrix which is tfidf_test_matrix_pca
# this allows me to apply various models
predict1 <- predict(forest1, tfidf_test_matrix_pca2, type = "response")
answer <- as.data.frame(table(predict1, class_labels_test))
answer$correct <- ifelse(answer$predict1==answer$class_labels_test, 1, 0)
tfidf_train_matrix <- pca_train$x[,1:194]
tfidf_train_matrix <- cbind(tfidf_train_matrix, class_labels_train)
loading_train <- pca_train$rotation[,1:194]
# multiply to get a test matrix with the principal component values
tfidf_test_matrix_pca <- scale(tfidf_test_matrix) %*% loading_train
tfidf_test_matrix_pca <- as.data.frame(tfidf_test_matrix_pca)
#tfidf_test_matrix_pca<-cbind(tfidf_test_matrix_pca, class_labels_test)
#tfidf_test_matrix_pca2<-as.data.frame(tfidf_test_matrix_pca)
#for (name in names(tfidf_test_matrix_pca2)){
#  if (name == "class_labels_test"){
#    next
# }else{
#    tfidf_test_matrix_pca2 [[name]] <- as.numeric(as.character(tfidf_test_matrix_pca2 [[name]]))
#  }
#}
#tfidf_test_matrix_pca2$class_labels_test <- as.factor(tfidf_test_matrix_pca2$class_labels_test)
tfidf_train_matrix2  <- as.data.frame(tfidf_train_matrix)
for (name in names(tfidf_train_matrix2 )){
if (name == "class_labels_train"){
next
}else{
tfidf_train_matrix2 [[name]] <- as.numeric(as.character(tfidf_train_matrix2 [[name]]))
}
}
tfidf_train_matrix2$class_labels_train <- as.factor(tfidf_train_matrix2$class_labels_train)
# i have training matrix tfidf_train_matrix2 and test matrix which is tfidf_test_matrix_pca
# this allows me to apply various models to predict author attribution
#RF is the simplest and my favourite to try as a starting model
forest1<-randomForest(class_labels_train ~ .,
data = tfidf_train_matrix2,
ntree = 500)
forest1<-randomForest(class_labels_train ~ .,
data = tfidf_train_matrix2,
ntree = 500)
predict1 <- predict(forest1, tfidf_test_matrix_pca, type = "response")
#MSE
predict1$mse[length(predict1$mse)]
actual<-tfidf_test_matrix_pca$class_labels_test
rmse<-mean((predict1-actual)^2)^0.5
print(rmse)
rmse
answer
predict1 <- predict(forest1, tfidf_test_matrix_pca, type = "response")
actual<-tfidf_test_matrix_pca$class_labels_test
rmse<-mean((predict1-actual)^2)^0.5
metrics_rmse=rmse(predict1, actual)
rmse
answer <- as.data.frame(table(predict1, class_labels_test))
answer
answer$correct <- ifelse(answer$predict1==answer$class_labels_test, 1, 0)
attributes(predict1)
attributes(class_labels_test)
dim(class_labels_test)
class_labels_test
